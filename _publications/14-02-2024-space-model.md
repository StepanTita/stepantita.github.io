---
title: "Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs"
collection: publications
permalink: /publication/14-02-2024-space-model
excerpt: 'In this paper, we present a framework that allows for maintaining generalizability, and enhances the performance on the downstream task by utilizing task-specific context attribution'
date: 14-02-2024
venue: 'AAAI Responsible Language Model (ReLM) Workshop'
paperurl: 'http://stepantita.github.io/files/SpaceModel.pdf'
citation: '@misc{tytarenko2024breaking,
      title={Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs}, 
      author={Stepan Tytarenko and Mohammad Ruhul Amin},
      year={2024},
      eprint={2401.16638},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}'
---
In this paper, we present a framework that allows for maintaining generalizability, and enhances the performance on the downstream task by utilizing task-specific context attribution

[Download paper here](http://stepantita.github.io/files/SpaceModel.pdf)

Recommended citation: ```@misc{tytarenko2024breaking,
      title={Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs}, 
      author={Stepan Tytarenko and Mohammad Ruhul Amin},
      year={2024},
      eprint={2401.16638},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}```
